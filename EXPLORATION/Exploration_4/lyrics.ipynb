{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "strong-temperature",
   "metadata": {},
   "source": [
    "# EXPLORATION 4 : 작사가 인공지능 만들기\n",
    "\n",
    "## 준비한 데이터\n",
    "- lyrics dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-mouth",
   "metadata": {},
   "source": [
    "## 1. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mineral-emerald",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['[Verse 1]', 'They come from everywhere', 'A longing to be free', 'They come to join us here', 'From sea to shining sea And they all have a dream', 'As people always will', 'To be safe and warm', 'In that shining city on the hill Some wanna slam the door', 'Instead of opening the gate', \"Aw, let's turn this thing around\"]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import re         # 정규표현식을 위한 모듈 (문장 데이터를 정돈) \n",
    "import numpy as np\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:                  # 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담기\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-fence",
   "metadata": {},
   "source": [
    "## 2. 데이터 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "every-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "        \n",
    "    sentence = sentence.lower().strip()                       # 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)       # 특수문자 양쪽에 공백을 넣고\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)               # 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)     # aa-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "    sentence = sentence.strip()                               # 다시 양쪽 공백을 지웁니다\n",
    "    sentence = '<start> ' + sentence + ' <end>'              # 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 문장이 필터링되는지 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adopted-revolution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150691"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:                 # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue        \n",
    "    if sentence[-1] == \":\": continue\n",
    "    if sentence[0] == \"[\" : continue\n",
    "    if sentence.count(\" \") > 10 : continue  \n",
    "\n",
    "    corpus.append(preprocess_sentence(sentence))       # 정제를 하고 담아주세요\n",
    "        \n",
    "len(corpus)       # 몇개가 담겼는지 확인하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-toronto",
   "metadata": {},
   "source": [
    "## 3. tokenize 패키기 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dependent-collector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   43   64 ...    0    0    0]\n",
      " [   2    9 2958 ...    0    0    0]\n",
      " [   2   43   64 ...    0    0    0]\n",
      " ...\n",
      " [   2  555   20 ...    0    0    0]\n",
      " [   2  121   36 ...    0    0    0]\n",
      " [   2    5   23 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f247f0d9d90>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(              \n",
    "        num_words=20000,                                            # 전체 단어의 개수 \n",
    "        filters=' ',                                                # 별도로 전처리 로직을 추가가능\n",
    "        oov_token=\"<unk>\"                                           # out-of-vocabulary\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)                                  # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "turkish-bankruptcy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-metabolism",
   "metadata": {},
   "source": [
    "## 4. 텐서 데이터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "humanitarian-arnold",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150691, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "polar-tiffany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   43   64   73  704    3    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2    9 2958   10   27  262    3    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2   43   64   10 2124  126   92    3    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2   73  544   10 1070  544    8   43   25   76    9  345    3    0\n",
      "     0]\n",
      " [   2   82  173  171   84    3    0    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2   10   27 1061    8  903    3    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2 1031   21 5583    6 2959    3    0    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2 1571    4   66   16  192   44  185  134    3    0    0    0    0\n",
      "     0]\n",
      " [   2  176   11  620  101  400  337    3    0    0    0    0    0    0\n",
      "     0]\n",
      " [   2   11   16   29   10   12    8    7    3    0    0    0    0    0\n",
      "     0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alpine-covering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exact-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  43  64  73 704   3   0   0   0   0   0   0   0   0]\n",
      "[ 43  64  73 704   3   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]   # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-lesbian",
   "metadata": {},
   "source": [
    "## 5. 평가 데이터셋 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "leading-freedom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (120552, 14)\n",
      "Target Train: (120552, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                         tgt_input,\n",
    "                                                         test_size=0.2,\n",
    "                                                         shuffle=True)\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-discretion",
   "metadata": {},
   "source": [
    "## 6. 훈련데이터셋 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "independent-belfast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((512, 14), (512, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 512\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 20000개와, 여기 포함되지 않은 0:<pad>를 포함하여 20001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-justice",
   "metadata": {},
   "source": [
    "## 7. 텍스트 제너레이터 모델 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "suspended-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 3072\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "legal-portrait",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(512, 14, 20001), dtype=float32, numpy=\n",
       "array([[[ 1.7306811e-04,  2.7666694e-05, -4.0732011e-05, ...,\n",
       "          1.0831895e-04,  4.6472007e-05,  1.9541067e-04],\n",
       "        [ 2.6668518e-04,  4.4235499e-06, -1.4309929e-04, ...,\n",
       "          4.7978610e-04,  7.6409582e-05,  3.3636941e-04],\n",
       "        [ 3.5137465e-04,  2.1702774e-04, -4.3320609e-04, ...,\n",
       "          6.6236535e-04, -3.1764564e-05,  6.3351175e-04],\n",
       "        ...,\n",
       "        [-1.7411254e-03,  1.1870423e-03,  3.9721778e-04, ...,\n",
       "         -1.4192265e-04, -1.1192209e-03,  2.9676426e-03],\n",
       "        [-1.9899332e-03,  1.4661695e-03,  5.4759311e-04, ...,\n",
       "         -2.4668136e-04, -1.1746883e-03,  3.2117842e-03],\n",
       "        [-2.1979348e-03,  1.7152413e-03,  6.9151731e-04, ...,\n",
       "         -3.4399095e-04, -1.2078827e-03,  3.4064411e-03]],\n",
       "\n",
       "       [[ 1.7306811e-04,  2.7666694e-05, -4.0732011e-05, ...,\n",
       "          1.0831895e-04,  4.6472007e-05,  1.9541067e-04],\n",
       "        [ 3.4548674e-04,  4.9869399e-05,  4.5183455e-04, ...,\n",
       "          2.2205610e-04, -2.3749354e-04,  3.4505563e-04],\n",
       "        [ 5.6417350e-04, -2.4450661e-04,  6.5331795e-04, ...,\n",
       "          3.2173085e-04, -2.5860549e-04,  6.1763742e-04],\n",
       "        ...,\n",
       "        [-9.3475718e-04,  4.2028452e-04, -2.8119230e-04, ...,\n",
       "         -7.0390364e-05, -7.0418214e-04,  8.9551485e-04],\n",
       "        [-1.2610942e-03,  7.4848108e-04, -2.0843139e-04, ...,\n",
       "         -1.1663478e-04, -7.6562952e-04,  1.4578365e-03],\n",
       "        [-1.5580168e-03,  1.0903407e-03, -9.9048244e-05, ...,\n",
       "         -1.6258766e-04, -8.3441910e-04,  1.9701584e-03]],\n",
       "\n",
       "       [[ 1.7306811e-04,  2.7666694e-05, -4.0732011e-05, ...,\n",
       "          1.0831895e-04,  4.6472007e-05,  1.9541067e-04],\n",
       "        [ 1.4550848e-05, -1.0209294e-04, -6.5510503e-05, ...,\n",
       "          2.0952467e-04, -1.2414351e-04,  3.0698393e-05],\n",
       "        [ 2.1629354e-05, -1.8625411e-04,  3.4740611e-04, ...,\n",
       "          2.4464662e-04, -4.6772102e-04, -6.8212132e-05],\n",
       "        ...,\n",
       "        [-5.5992824e-04,  1.1486975e-03,  7.1843859e-04, ...,\n",
       "          2.2205684e-04, -1.1077748e-03,  5.6557299e-04],\n",
       "        [-9.4057078e-04,  1.3707218e-03,  7.3212181e-04, ...,\n",
       "          1.8920984e-04, -1.2251377e-03,  1.1087877e-03],\n",
       "        [-1.3081576e-03,  1.6010720e-03,  7.6844468e-04, ...,\n",
       "          1.5370258e-04, -1.3304320e-03,  1.6215170e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.7306811e-04,  2.7666694e-05, -4.0732011e-05, ...,\n",
       "          1.0831895e-04,  4.6472007e-05,  1.9541067e-04],\n",
       "        [ 2.1019735e-04,  9.5899115e-05, -3.1831057e-04, ...,\n",
       "          3.1916710e-04,  1.5338656e-04,  2.4594885e-04],\n",
       "        [ 1.8753584e-04,  1.6920832e-04, -5.7008822e-04, ...,\n",
       "          4.8389941e-04,  1.6918765e-04, -9.2626046e-08],\n",
       "        ...,\n",
       "        [-3.1164626e-04,  4.5270342e-04,  7.0135895e-04, ...,\n",
       "          3.8916676e-04, -1.5478216e-04, -5.3380785e-04],\n",
       "        [-7.0316880e-04,  6.1535154e-04,  6.5454468e-04, ...,\n",
       "          3.7938540e-04, -2.2547090e-04, -1.9570974e-05],\n",
       "        [-1.1113426e-03,  8.2886289e-04,  6.1989774e-04, ...,\n",
       "          3.8551085e-04, -3.2325808e-04,  5.3838413e-04]],\n",
       "\n",
       "       [[ 1.7306811e-04,  2.7666694e-05, -4.0732011e-05, ...,\n",
       "          1.0831895e-04,  4.6472007e-05,  1.9541067e-04],\n",
       "        [ 2.1813145e-04,  2.3994059e-04,  4.8357488e-05, ...,\n",
       "          1.5700297e-04,  3.5835201e-05,  3.5720522e-04],\n",
       "        [ 5.9804202e-05,  5.8283389e-05,  1.3598335e-04, ...,\n",
       "          9.6344840e-05, -8.3842562e-05,  4.7732412e-04],\n",
       "        ...,\n",
       "        [-1.6051057e-03,  1.5482384e-03,  8.7870721e-04, ...,\n",
       "         -4.6479763e-04, -1.7752883e-03,  2.0729261e-03],\n",
       "        [-1.8533765e-03,  1.8081317e-03,  9.6517068e-04, ...,\n",
       "         -4.8441990e-04, -1.7270683e-03,  2.4161760e-03],\n",
       "        [-2.0639221e-03,  2.0333068e-03,  1.0518908e-03, ...,\n",
       "         -5.1406788e-04, -1.6757504e-03,  2.7063121e-03]],\n",
       "\n",
       "       [[ 1.7306811e-04,  2.7666694e-05, -4.0732011e-05, ...,\n",
       "          1.0831895e-04,  4.6472007e-05,  1.9541067e-04],\n",
       "        [ 1.9488830e-04, -3.9467857e-05,  2.4417366e-04, ...,\n",
       "          1.2526447e-04,  4.5830417e-05,  6.0993887e-04],\n",
       "        [ 2.6359057e-04,  1.5435465e-04,  5.6831463e-04, ...,\n",
       "         -6.1825973e-05,  5.2500174e-05,  8.9104852e-04],\n",
       "        ...,\n",
       "        [-1.7359507e-03,  1.6086970e-03,  8.7864103e-04, ...,\n",
       "         -1.8067936e-04, -8.4222161e-04,  3.0361840e-03],\n",
       "        [-1.9876894e-03,  1.8929038e-03,  9.3287439e-04, ...,\n",
       "         -2.4181303e-04, -9.2251593e-04,  3.2539421e-03],\n",
       "        [-2.2026540e-03,  2.1317210e-03,  1.0006011e-03, ...,\n",
       "         -3.1015542e-04, -9.7822200e-04,  3.4277388e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "detailed-august",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  10240512  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  44052480  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  75509760  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  61463073  \n",
      "=================================================================\n",
      "Total params: 191,265,825\n",
      "Trainable params: 191,265,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-split",
   "metadata": {},
   "source": [
    "## 8. 모델훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ultimate-research",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 [==============================] - 926s 4s/step - loss: 4.3343\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 933s 4s/step - loss: 3.1286\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 935s 4s/step - loss: 2.8960\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 935s 4s/step - loss: 2.7308\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 935s 4s/step - loss: 2.5723\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 935s 4s/step - loss: 2.4203\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 934s 4s/step - loss: 2.2823\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 933s 4s/step - loss: 2.1402\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 932s 4s/step - loss: 2.0247\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 932s 4s/step - loss: 1.8967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2470f33250>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-pepper",
   "metadata": {},
   "source": [
    "## 9. 테스트 데이터 셋 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "excess-paste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((512, 14), (512, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_val)\n",
    "BATCH_SIZE = 512\n",
    "steps_per_epoch = len(enc_val) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 20000개와, 여기 포함되지 않은 0:<pad>를 포함하여 20001개\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-spectacular",
   "metadata": {},
   "source": [
    "## 10. 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "annual-membership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 79s 1s/step - loss: 2.4285\n"
     ]
    }
   ],
   "source": [
    "results=model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-fisher",
   "metadata": {},
   "source": [
    "## 11. 문장생성기 구축하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "standard-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=15):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-convertible",
   "metadata": {},
   "source": [
    "## 12. 문장 생성해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "collectible-chess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you know i m bad , i m bad shameone <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "chicken-institution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i m a survivor <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "chief-listing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> can t be now <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> can\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-working",
   "metadata": {},
   "source": [
    "# 프로젝트 결과\n",
    "### 느낀점\n",
    "1. hidden_size = 3072 , embedding_size = 512 로 바꿔서 진행하니까 토탈 파라미터의 수가 1억 9천개가 넘어갔고, 한 에폭당 15분이 넘도록 걸려서 너무 오래 기다렸지만 결과적으로 문장이 잘 생성되어서 다행이었다.\n",
    "2. 코드 하나하나의 의미를 분석해보려고 최대한 노력해보았는데 자연어처리는 처음이어서 어려운 부분이 많았다. RNN에 대해 공부의 필요성을 느꼈다.\n",
    "3. 정규표현식이 자연어처리에서 필수적이라는 생각이 들었고 자연어처리뿐만 아니라 다양한 곳에서 필요하기 때문에 한번 날을 잡아서 공부해야겠다는 생각이 들었다.\n",
    "4. 토크나이즈, 텐서에 대해 공부를 하면서 자연어처리의 흐름에 대해 좀 더 알 수 있어서 좋았다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
